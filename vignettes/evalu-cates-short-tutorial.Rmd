---
title: "Short Tutorial"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Short Tutorial}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)

library(evaluCATE)
library(dplyr)
library(grf)
library(ggplot2)
library(gridExtra)
library(reshape2)
```

In this tutorial, we show how to use the `evaluCATE` package to evaluate the quality of our conditional average treatment effect (CATE) estimates. Before diving in the technical details, we need to define notation:

- $Y_i$ &rarr; Observed outcome;
- $D_i \in \{ 0, 1\}$ &rarr; Treatment indicator;
- $X_i$ &rarr; Covariate vector;
- $\mu ( X_i ) = \mathbb{E} [ Y_i | X_i ]$ &rarr; Conditional mean of $Y_i$ given $X_i$;
- $\mu (d, X_i ) = \mathbb{E} [ Y_i | X_i, D_i = d ]$ &rarr; Conditional mean of $Y_i$ given $X_i$ among those units such as $D_i = d$;
- $p ( X_i ) = \mathbb{P} [ D_i = 1 | X_i ]$ &rarr; Propensity score;
- $w ( X_i ) = \frac{1}{p ( X_i ) [ 1 - p ( X_i )]}$ &rarr; Propensity score weights;
- $H_i = w ( X_i ) [ D_i - p (X_i) ]$ &rarr; Horvitz-Thompson operator;
- $\Gamma_i := \mu_1 ( X_i ) - \mu_0 ( X_i ) + \frac{D_i [ Y_i - \mu_1 ( X_i ) ]}{p ( X_i )} - \frac{[ 1 - D_i ] [ Y_i - \mu_0 ( X_i ) ]}{1 - p ( X_i )}$  &rarr; Doubly-robust scores;
- $\tau = \mathbb{E} [ Y_i ( 1 ) - Y_i ( 0 ) ]$ &rarr; Average treatment effect (ATE);
- $\tau ( X_i ) = \mathbb{E} [ Y_i ( 1 ) - Y_i ( 0 ) | X_i ]$ &rarr; CATEs.

Throughout the rest of the tutorial, we assume SUTVA and unconfoundedness (e.g., Imbens and Rubin, 2015).

## Motivating Example
We illustrate the purpose of the `evaluCATE` package through the following example, where we generate simulated data. In this data, we have only one covariate $X_{i, 1} \sim \mathcal{U} ( 0, 1 )$, the treatment is randomly allocated as in a Bernoulli experiment, and the treatment effects are zero for all units. 

```{r generate-data, eval = TRUE}
## Generate data.
set.seed(1986)

n <- 500
k <- 1

X <- matrix(runif(n * k), ncol = k)
colnames(X) <- paste0("x", seq_len(k))
D <- rbinom(n, size = 1, prob = 0.5)
mu0 <- X[, 1]
mu1 <- X[, 1]
Y <- mu0 + D * (mu1 - mu0) + rnorm(n)
```

We now proceed to estimate the CATEs using an honest causal forest. To achieve this, we divide our data set into two subsamples: a training sample for estimation and a validation sample to check our results. The latter task is achieved by generating two plots: kernel density of the out-of-sample CATE predictions, and out-of-sample CATE predictions arranged in ascending order together with pointwise $95\%$ confidence intervals. To enhance visualization, we smooth the standard errors by a Nadaraya-Watson regression.

```{r cates-estimation, eval = TRUE, fig.show = 'hold', fig.dim = c(5, 3)}
## Sample split.
train_idx <- sample(c(TRUE, FALSE), length(Y), replace = TRUE)

X_tr <- matrix(X[train_idx, ])
X_val <- matrix(X[!train_idx, ])

D_tr <- D[train_idx]
D_val <- D[!train_idx]

Y_tr <- Y[train_idx]
Y_val <- Y[!train_idx]

## CATEs estimation.
forest <- causal_forest(X_tr, Y_tr, D_tr) # We use only the training sample.
forest_predictions <- predict(forest, X, estimate.variance = TRUE) # We predict on the full sample.

cates <- forest_predictions$predictions 
se <- sqrt(forest_predictions$variance.estimates)

## Arrange plot data and smooth standard errors.
plot_dta <- data.frame("estimated_cates" = cates, "se" = se)
plot_dta <- plot_dta[order(plot_dta$estimated_cates, decreasing = FALSE), ]

plot_dta$smoothed_se <- ksmooth(1:dim(plot_dta)[1], plot_dta$se, bandwidth = 100, kernel = "normal")$y

plot_dta$CIL <- plot_dta$estimated_cates - 1.96 * plot_dta$smoothed_se
plot_dta$CIU <- plot_dta$estimated_cates + 1.96 * plot_dta$smoothed_se

## Plot out-of-sample predicted CATEs.
kernel_plot <- plot_dta[!train_idx, ] %>%
  ggplot(aes(x = estimated_cates)) +
  geom_density(color = "black", fill = "dodgerblue", alpha = 0.4, linewidth = 1) + 
  xlab("Estimated CATEs") + ylab("Density") +
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5), legend.position = "none")

sorted_plot <- plot_dta[!train_idx, ] %>%
  ggplot(aes(x = 1:(n - sum(train_idx)), y = estimated_cates)) + 
  geom_line(linewidth = 1, color = "black") + 
  geom_hline(yintercept = 0, linetype = "dashed", linewidth = 0.7) + 
  geom_ribbon(aes(ymin = CIL, ymax = CIU), fill = "dodgerblue", linetype = 0, alpha = 0.5) + 
  xlab("Units") + ylab("Estimated CATEs") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5), legend.position = "none")
        
grid.arrange(kernel_plot, sorted_plot, ncol = 2)
```

One could look at the left panel of the figure above and conclude that there is heterogeneity in the treatment effects. However, high variation in the predictions does not necessarily imply that the effects are heterogeneous, as it could be that estimation noise is messing with our results (exactly what is happening in our example!). One could also look at right panel of the figure above and conclude that the effect is zero for all units, which is true in our example but in real applications we would not know whether this is driven again by estimation noise. 

These complications arise because applying machine learning tools to the estimation of heterogeneous treatment effects may produce low-quality estimates of the CATEs, as shown in the next figure where we overlay the true and the estimated CATEs.

```{r true-estimated-cates, eval = TRUE, fig.show = 'hold', fig.dim = c(5, 3)}
## Plot true and estimated CATEs.
data.frame("x1" = X, "true_cates" = mu1 - mu0, "estimated_cates" = cates) %>%
  melt(id.vars = "x1") %>%
  ggplot(aes(x = x1, y = value, group = variable, color = variable)) + 
  geom_line(linewidth = 1) +
  scale_color_manual(name = "", labels = c("True", "Estimated"), values = c("tomato", "dodgerblue")) +
  xlab("X1") + ylab("CATEs") +  
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5), legend.position = c(0.13, 0.85), legend.text = element_text(size = 8), legend.key.size = unit(0.4, 'cm'))
```

It is thus crucial to rely on appropriate procedures to evaluate the quality of the estimated CATEs and assess whether systematic heterogeneity is detected. This is exactly the purpose of the `evaluCATE` package. 

## Quality Evaluation
The quality evaluation of the estimated CATEs is performed by targeting three key features of the CATEs:

- Best Linear Predictor (BLP) of the actual CATEs using the estimated CATEs;
- Sorted Group Average Treatment Effects (GATES).
- Rank-Weighted Average Treatment Effects (RATEs) induced by the estimated CATEs.

This section provides an overview of these targets. The discussion is loosely based on Chernozhukov et al. (2017), Yadlowsky et al. (2021), and Imai and Li (2022).[^1]

### Best Linear Predictor
The BLP of the actual CATEs using the estimated CATEs is defined as follows:

$$ BLP [\tau ( X_i ) | \hat{\tau} ( X_i )] := \beta_1 + \beta_2 [ \hat{\tau} ( X_i ) - \mathbb{E} [ \hat{\tau} ( X_i ) ] ]$$

with $\beta_1 = \mathbb{E} [ \tau ( X_i ) ]$ and $\beta_2 = Cov [ \tau ( X_i ), \hat{\tau} ( X_i ) ] / Var [ \hat{\tau} ( X_i ) ]$. 

We are interested in the estimation of the BLP for two main reasons: 

- $\hat{\beta}_1$ provides an estimate of the ATE;
- $\beta_2 = 0$ if either the effects are homogeneous or our CATE estimates are "bad." Thus, rejecting the hypothesis $\beta_2 = 0$ would imply that both the effects are heterogeneous and our CATE estimates are reliable.[^2]

The `evaluCATE` package estimates the BLP using three different strategies, each involving fitting a suitable linear model.[^3] To achieve valid inference, we require a training-validation sample split, as we did in the Motivating Example section. Then, estimation of the following is performed using only the validation sample.[^4]

##### BLP - **Weighted Residuals**:

$$ Y_i = \beta_1 [ D_i - p ( X_i ) ] + \beta_2 \{ [ D_i - p ( X_i ) ] [ \hat{\tau} ( X_i ) - \mathbb{E}_n [ \hat{\tau} ( X_i ) ] ] \} + \epsilon_i $$
with the model fitted via WLS using weights $w ( X_i)$ and $\mathbb{E}_n$ denoting the sample average operator. 

##### BLP - **Horvitz-Thompson**

$$ H_i Y_i = \beta_1 + \beta_2 \{ \hat{\tau} ( X_i ) - \mathbb{E}_{n, V} [ \hat{\tau} ( X_i ) ] \} + \epsilon_i $$
with the model fitted via OLS. 

##### BLP - **AIPW**

$$ \hat{\Gamma}_i = \beta_1 + \beta_2 \{ \hat{\tau} ( X_i ) - \mathbb{E}_{n, V} [ \hat{\tau} ( X_i ) ] \} + \epsilon_i $$
with the model fitted via OLS and the doubly-robust scores $\Gamma_i$ estimated using the validation sample and 5-fold cross-fitting.

### Sorted Group Average Treatment Effects
The GATES are defined as follows: 

$$ \gamma_k := \mathbb{E} [ \tau ( X_i ) | \hat{\ell}_{k - 1} \leq \hat{\tau} ( X_i ) < \hat{\ell}_k ], \,\,\, k = 1, \dots, K$$

with the groups formed by cutting the distribution of $\hat{\tau} ( \cdot )$ into $K$ bins using the empirical quantiles of $\hat{\tau} ( \cdot )$ $\{ \hat{\ell}_k \}_{k = 1}^K$. 

We are interested in the estimation of the GATES for two main reasons:

- They quantify the extent to which the effects differ across groups;
- They allow us to assess whether we detect systematic heterogeneity or just estimation noise by testing a set of hypotheses (check the [hypotheses testing vignette](https://riccardo-df.github.io/evaluCATE/articles/hypotheses-testing.html) for details).

The `evaluCATE` package estimates the GATES using four different strategies: three of them involve fitting a suitable linear model, while the fourth hinges on a nonparametric estimator. To achieve valid inference, we require a training-validation sample split, as we did in the Motivating Example section. Then, estimation of the following is performed using only the validation sample.[^5]

##### GATES - **Weighted Residuals**

$$ Y_i = \sum_{k = 1}^K \gamma_k [ D_i - p ( X_i ) ] \mathbb{1} ( \hat{\ell}_{k - 1} \leq \hat{\tau} ( X_i ) < \hat{\ell}_k ) + \epsilon_i $$

with the model fitted via WLS using weights $w ( X_i)$.

##### GATES - **Horvitz-Thompson**

$$ H_i Y_i = \sum_{k = 1}^K \gamma_k \mathbb{1} ( \hat{\ell}_{k - 1} \leq \hat{\tau} ( X_i ) < \hat{\ell}_k ) + \epsilon_i $$

with the model fitted via OLS.

##### GATES - **AIPW**

$$ \hat{\Gamma}_i = \sum_{k = 1}^K \gamma_k \mathbb{1} ( \hat{\ell}_{k - 1} \leq \hat{\tau} ( X_i ) < \hat{\ell}_k ) + \epsilon_i $$

with the model fitted via OLS and the doubly-robust scores $\Gamma_i$ estimated using the validation sample and 5-fold cross-fitting.

##### GATES - **Nonparametric**

$$ \hat{\gamma}_k = \frac{K}{\sum_{i = 1}^n D_i} \sum_{i = 1}^n Y_i D_i \mathbb{1} ( \hat{\ell}_{k - 1} \leq \hat{\tau} ( X_i ) < \hat{\ell}_k ) - \frac{K}{\sum_{i = 1}^n [ 1 - D_i ]} \sum_{i = 1}^n Y_i [ 1 - D_i ] \mathbb{1} ( \hat{\ell}_{k - 1} \leq \hat{\tau} ( X_i ) < \hat{\ell}_k ) $$

### Rank-Weighted Average Treatment Effects
The RATE induced by the estimated CATEs is defined as follows:

$$ \theta_{\alpha} ( \hat{\tau} ) := \int_0^1 \alpha ( u ) TOC ( u; \hat{\tau} ) d u  $$
where:

$$ TOC (u; \hat{\tau}) := \mathbb{E} [ Y_i ( 1 ) - Y_i ( 0 ) | F ( \hat{\tau} ( X_i ) \geq 1 - u) ] - \mathbb{E} [ Y_i ( 1 ) - Y_i ( 0 ) ] $$
with $F ( \cdot )$ the cumulative distribution function of $\hat{\tau} ( \cdot )$, $0 < u \leq 1$, and $\alpha : ( 0, 1 ] \rightarrow \mathcal{R}$ a generic weight function.

The RATE provides a measure of the ability of our estimated CATEs to prioritize units to treatment in terms of intervention benefit. The idea is to regard $\hat{\tau} ( \cdot )$ as a "prioritization rule" that sorts units $i = 1, ..., n$ in order $j = 1, ..., n$ according to their estimated CATEs, for instance by prioritizing units with the largest estimated CATEs.[^6] [^7] 

We are interested in the estimation of the RATE for two main reasons:

- $\theta_{\alpha} ( \hat{\tau}_1 ) > \theta_{\alpha} ( \hat{\tau}_2 )$ means that $\hat{\tau}_1 ( \cdot )$ produces more accurate CATE estimates than $\hat{\tau}_2 ( \cdot )$; 
- $\theta_{\alpha} ( \hat{\tau} ) = 0$ if either the effects are homogeneous or our CATE estimates are “bad." Thus, rejecting the hypothesis $\theta_{\alpha} ( \hat{\tau} ) = 0$ would imply that both the effects are heterogeneous and our CATE estimates are reliable.[^8]

The `evaluCATE` package estimates the TOCs and the RATE using the following sample-averaging estimators:

$$ \widehat{TOC} ( u; \hat{\tau} ) = \frac{1}{ \lfloor u n \rfloor } \sum_{j = 1}^{\lfloor u n \rfloor} \hat{\Gamma}_{i ( j )} - \frac{1}{n} \sum_{i = 1}^n \hat{\Gamma}_i $$
$$ \hat{\theta}_{\alpha} ( \hat{\tau} ) = \frac{1}{n} \sum_{j = 1}^n \alpha ( \frac{j}{n} ) \widehat{TOC} ( \frac{j}{n}; \hat{\tau} ) $$
where we let $i ( j )$ be the mapping from rank $j$ to unit $i$ (e.g., $i ( 1 )$ returns the most-prioritized unit, and $i ( n )$ returns the least-prioritized unit) and the doubly-robust scores $\Gamma_i$ are estimated using the validation sample and 5-fold cross-fitting. Two different weight functions are considered, each corresponding to a different RATE:

- $\alpha ( u ) = 1$ &rarr; Area under the TOC curve (AUTOC)
- $\alpha ( u ) = u$ &rarr; Qini coefficient (QINI)

The half-sample bootstrap procedure is used to estimate the standard error of $\hat{\theta}_{\alpha} ( \cdot )$. In particular, the standard deviation of the bootstrap estimates can be used as an estimator of the standard error of $\hat{\theta}_{\alpha} ( \cdot )$.

## Code
The BLP, GATES, and RATEs can be estimated by calling the `evalu_cates` function. When calling this function, we need to supply the full sample using the first three arguments: `Y`, `D`, and `X`, corresponding to $Y_i$, $D_i$, and $X_i$. Additionally, we must supply our CATE predictions on the full sample $\hat{\tau} ( X_i )$ obtained using only the training sample by using the `cates` argument. Finally, we must supply a logical vector with the `TRUE`s denoting those observations used to estimate the CATEs by using the `is_train` argument, so that the `evalu_cates` function knows which observations must be used to post-process our CATEs estimates.

We also have four optional arguments that we can use to supply estimates of the nuisance functions $p ( \cdot )$, $\mu ( \cdot )$, $\mu_0 ( \cdot )$, and $\mu_1 ( \cdot )$. Be careful, as these estimates must be obtained using only the training sample. If not provided by the user, these functions are estimated internally via honest regression forests using only the training sample. In our Motivating Example, we have knowledge of the actual propensity score, which equals $0.5$ for all units. We supply these values in the call below and let the function estimate the other nuisances internally.[^9] 

Finally, we have four additional optional arguments. The first of these arguments controls the number of groups to be formed for the GATES analysis, with the default number equal to five. The second of these arguments controls the number of bootstrap replications used to estimate the standard error of the estimated RATEs. The third of these arguments controls how to rank units for the RATE estimation (according to either increasing or decreasing values of the estimated CATEs). The fourth of these arguments controls whether the `evalu_cates` function should print the status of progress on the console. We use the default of 5 groups, 200 bootstrap replications, the treatment is considered to be beneficial, and we prevent the function from printing the progresses.  

```{r call-main, eval = TRUE}
## Call main function.
pscore <- rep(0.5, length(Y)) # True propensity scores.
evaluation <- evalu_cates(Y, D, X, cates, train_idx, pscore = pscore, verbose = FALSE)
```

Let us have a look at the results. The `summary` method allows us to visualize the results of the BLP and RATE estimation if the `target` argument is set to `BLP` or the results of the GATES estimation if the `target` argument is set to `GATES`. The `latex` argument controls whether the raw results or LATEX code for a table will be displayed in the console.

```{r blp-rates summary, eval = TRUE}
## BLP summary.
summary(evaluation, target = "BLP") # Try 'latex = TRUE'.
```

The estimated ATEs range between $0.05$ and $0.18$, with all confidence intervals covering the true value computed as `mean(mu1 - mu0)`. Additionally, we always fail to reject the hypothesis $\beta_2 = 0$, implying that either the effects are homogeneous or our CATE estimates are unreliable (we know both to be true in our example). The estimated RATEs agree with these findings.

```{r gates-summary, eval = TRUE}
## GATES summary.
summary(evaluation, target = "GATES") # Try 'latex = TRUE'.
```

All the GATES confidence intervals include the zero, suggesting that no group is affected by the treatment (true in our example!). The $p$-values attached to testing the hypotheses that all the GATES are equal and that the most affected and the least affected groups feature the same response to the treatment are large and lead to a failure in rejecting these hypotheses. To better digest GATES point estimates and confidence intervals, we can use the `plot` method setting the `target` argument to `"GATES"`.

```{r gates-plot, eval = TRUE, fig.dim = c(5, 3)}
## GATES plot.
plot(evaluation, target = "GATES")

```

Finally, we can look at the estimated TOC curve by calling the `plot` method setting the `target` argument to `"TOC"`. 

```{r toc-plot, eval = TRUE, fig.dim = c(5, 3)}
## TOC curve.
plot(evaluation, target = "TOC")
```

[^1]: Complete references to these papers are listed in the home page.
[^2]: Failing to reject this hypothesis means that either there is no heterogeneity or our CATE estimates are not reliable. Without additional evidence, we are not able to disentangle this. Check the [hypotheses testing vignette](https://riccardo-df.github.io/evaluCATE/articles/hypotheses-testing.html) for more details. 
[^3]: The linear regressions are used for estimation purposes. The identification hinges on linear projections defined at the population level, with the linear regressions constituting their sample analogs.
[^4]: Additional constructed covariates which are not necessary for identifying the targets but can significantly reduce the variance of the estimation can be included in the regressions. Details can be found in the [denoising vignette](https://riccardo-df.github.io/evaluCATE/articles/denoising.html).
[^5]: See footnotes 3 and 4.
[^6]: If the treatment is harmful, we prioritize units with the lowest estimated CATEs.
[^7]: Prioritization rules can be derived from other approaches, e.g., risk-based rules. Here we focus on CATE-based rules as we aim to use the RATEs to evaluate the quality of our estimated CATEs.
[^8]: See footnote 2.
[^9]: Notice that most methodologies implemented here are valid only under randomized experiments, where $p ( \cdot )$ is known.
