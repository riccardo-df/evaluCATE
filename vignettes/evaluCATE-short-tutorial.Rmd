---
title: "Short Tutorial"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Short Tutorial}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)

library(evaluCATE)
library(dplyr)
library(grf)
library(ggplot2)
library(reshape2)
```

In this tutorial, we show how to use the `evaluCATE` package to evaluate your conditional average treatment effect (CATE) estimates. Before diving in the technical details, we need to define notation:

- $Y_i$ &rarr; Observed outcome;
- $D_i \in \{ 0, 1\}$ &rarr; Treatment indicator;
- $X_i$ &rarr; Covariate vector;
- $\mu ( X_i ) := \mathbb{E} [ Y_i | X_i ]$ &rarr; Conditional mean of $Y_i$ given $X_i$;
- $\mu (d, X_i ) := \mathbb{E} [ Y_i | X_i, D_i = d ]$ &rarr; Conditional mean of $Y_i$ given $X_i$ for subgroup $D_i = d$;
- $p ( X_i ) := \mathbb{P} [ D_i = 1 | X_i ]$ &rarr; Propensity score;
- $w ( X_i ) := \frac{1}{p ( X_i ) [ 1 - p ( X_i )]}$ &rarr; Propensity score weights;
- $H_i := w ( X_i ) [ D_i - p (X_i) ]$ &rarr; Horvitz-Thompson operator;
- $\Gamma_i := \mu_1 ( X_i ) - \mu_0 ( X_i ) + \frac{D_i [ Y_i - \mu_1 ( X_i ) ]}{p ( X_i )} - \frac{[ 1 - D_i ] [ Y_i - \mu_0 ( X_i ) ]}{1 - p ( X_i )}$  &rarr; Doubly-robust score;
- $\tau := \mathbb{E} [ Y_i ( 1 ) - Y_i ( 0 ) ]$ &rarr; Average treatment effect (ATE);
- $\tau ( X_i ) := \mathbb{E} [ Y_i ( 1 ) - Y_i ( 0 ) | X_i ]$ &rarr; CATE.

Throughout the rest of the tutorial, we assume SUTVA and unconfoundedness (e.g., Imbens and Rubin, 2015).

## Motivating Example
We illustrate the usage of the `evaluCATE` package with simulated data:

- $X_{i} \sim \mathcal{U} ( 0, 1 )$ single uniformly distributed covariate

- $D_i \sim Bernoulli(1/2)$ randomly assigned treatment

- $Y_i = X_i + 0 \times D_i + \varepsilon_i$ with $\varepsilon_i \sim \mathcal{N} ( 0, 1 )$

This implies a homogeneous zero effect, i.e. $\tau ( X_i ) = 0$

```{r generate-data, eval = TRUE}
## Generate data.
set.seed(1986)

n <- 500
k <- 1

X <- matrix(runif(n * k), ncol = k)
colnames(X) <- paste0("x", seq_len(k))
D <- rbinom(n, size = 1, prob = 0.5)
mu0 <- X[, 1]
mu1 <- X[, 1]
Y <- mu0 + D * (mu1 - mu0) + rnorm(n)
```

We now proceed to estimate the CATEs using the causal forest estimator. We divide our data set into two subsamples: 

1. a training sample for estimation of CATEs, and

2. a validation sample to evaluate the estimated heterogeneity.

```{r cates-estimation, eval = TRUE}
## Sample split.
train_idx <- sample(c(TRUE, FALSE), length(Y), replace = TRUE)

X_tr <- matrix(X[train_idx, ])
X_val <- matrix(X[!train_idx, ])

D_tr <- D[train_idx]
D_val <- D[!train_idx]

Y_tr <- Y[train_idx]
Y_val <- Y[!train_idx]

## CATEs estimation.
forest <- causal_forest(X_tr, Y_tr, D_tr) # We use only the training sample.
cates_val <- predict(forest, X_val)$predictions # We predict on the validation sample.
```

We then display the out-of-sample estimated CATEs distribution: 

```{r cates-plot, eval = TRUE, fig.show = 'hold', fig.dim = c(5, 3)}
## Plot out-of-sample predicted CATEs.
data.frame("cates" = cates_val) %>%
  ggplot(aes(x = cates)) +
  geom_histogram(color = "black", fill = "dodgerblue", alpha = 0.4, bins = 10) + 
  xlab("Estimated CATEs") + ylab("Density") +
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5), legend.position = "none")
```

One could look at the histogram and conclude that there is heterogeneity in the treatment effects. However, high variation in the predictions does not necessarily imply that the effects are heterogeneous, as it could be that estimation noise is messing with our results (exactly what is happening in our example!). These complications arise because applying machine learning tools to the estimation of heterogeneous treatment effects may produce low-quality estimates of the CATEs, as shown in the next figure where we overlay the true and the estimated CATEs.

```{r true-estimated-cates, eval = TRUE, fig.show = 'hold', fig.dim = c(5, 3)}
## Plot true and estimated CATEs.
data.frame("x1" = X_val, "true_cates" = mu1[!train_idx] - mu0[!train_idx], "estimated_cates" = cates_val) %>%
  melt(id.vars = "x1") %>%
  ggplot(aes(x = x1, y = value, group = variable, color = variable)) + 
  geom_line(linewidth = 1) +
  scale_color_manual(name = "", labels = c("True", "Estimated"), values = c("tomato", "dodgerblue")) +
  xlab("X1") + ylab("CATEs") +  
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5), legend.position = c(0.13, 0.85), legend.text = element_text(size = 8), legend.key.size = unit(0.4, 'cm'))
```

It is thus crucial to rely on appropriate procedures to evaluate the quality of the estimated CATEs and assess whether systematic heterogeneity is detected. This is the purpose of the `evaluCATE` package. 

## Quality Evaluation
The quality evaluation of the estimated CATEs is performed by targeting three key features of the CATEs:

- Best Linear Predictor (BLP) of the actual CATEs using the estimated CATEs;
- Sorted Group Average Treatment Effects (GATES).
- Rank-Weighted Average Treatment Effects (RATEs) induced by the estimated CATEs.

This section provides an overview of these targets. The discussion is loosely based on Chernozhukov et al. (2017), Yadlowsky et al. (2021), and Imai and Li (2022).[^1]

### Best Linear Predictor
The BLP of the actual CATEs using the estimated CATEs is defined as follows:

$$ BLP [\tau ( X_i ) | \hat{\tau} ( X_i )] := \beta_1 + \beta_2 [ \hat{\tau} ( X_i ) - \mathbb{E} [ \hat{\tau} ( X_i ) ] ]$$

with $\beta_1 = \mathbb{E} [ \tau ( X_i ) ]$ and $\beta_2 = Cov [ \tau ( X_i ), \hat{\tau} ( X_i ) ] / Var [ \hat{\tau} ( X_i ) ] = E\left[\frac{\hat{\tau} ( X_i )- E[\hat{\tau} ( X_i )]}{Var [ \hat{\tau} ( X_i ) ]} \tau ( X_i ) \right]$. 

We are interested in the estimation of the BLP for two main reasons: 

- $\hat{\beta}_1$ provides an estimate of the ATE;
- $\beta_2 = 0$ if either the effects are homogeneous or our CATE estimates are "bad." Thus, rejecting the hypothesis $\beta_2 = 0$ would imply that both the effects are heterogeneous and our CATE estimates are reliable.[^2]

The `evaluCATE` package estimates the BLP using three different strategies, each involving fitting a suitable regression model.[^3] To achieve valid inference, we require a training-validation sample split, as we did in the Motivating Example section. Then, estimation of the BLP is performed in the validation sample.[^4] We consider three alternatives to estimate BLP:

##### BLP - **Weighted Residuals**:

$$ Y_i = \beta_1 [ D_i - p ( X_i ) ] + \beta_2 \{ [ D_i - p ( X_i ) ] [ \hat{\tau} ( X_i ) - \mathbb{E}_n [ \hat{\tau} ( X_i ) ] ] \} + \epsilon_i $$
with the model fitted via WLS using weights $w ( X_i)$ and $\mathbb{E}_n$ denoting the sample average operator. 

##### BLP - **Horvitz-Thompson**

$$ H_i Y_i = \beta_1 + \beta_2 \{ \hat{\tau} ( X_i ) - \mathbb{E}_{n, V} [ \hat{\tau} ( X_i ) ] \} + \epsilon_i $$
with the model fitted via OLS. 

##### BLP - **AIPW**

$$ \hat{\Gamma}_i = \beta_1 + \beta_2 \{ \hat{\tau} ( X_i ) - \mathbb{E}_{n, V} [ \hat{\tau} ( X_i ) ] \} + \epsilon_i $$
with the model fitted via OLS and the doubly-robust scores $\Gamma_i$ estimated via cross-fitting in the validation sample.

### Sorted Group Average Treatment Effects
The GATES are defined as follows: 

$$ \gamma_k := \mathbb{E} [ \tau ( X_i ) | \hat{\ell}_{k - 1} \leq \hat{\tau} ( X_i ) < \hat{\ell}_k ], \,\,\, k = 1, \dots, K$$

with the groups formed by cutting the distribution of $\hat{\tau} ( \cdot )$ into $K$ bins using the empirical quantiles of $\hat{\tau} ( \cdot )$ $\{ \hat{\ell}_k \}_{k = 1}^K$. 

We are interested in the estimation of the GATES for two main reasons:

- They quantify the extent to which the effects differ across groups;
- They allow us to assess whether we detect systematic heterogeneity or just estimation noise by testing a set of hypotheses (check the [hypotheses testing vignette](https://riccardo-df.github.io/evaluCATE/articles/hypotheses-testing.html) for details).

The `evaluCATE` package estimates the GATES using four different strategies: three of them involve fitting a suitable linear model, while the fourth hinges on a nonparametric estimator. To achieve valid inference, we require a training-validation sample split, as we did in the Motivating Example section. Then, estimation of the GATES is performed using only the validation sample.[^5] We consider four alternatives to estimate GATES:

##### GATES - **Weighted Residuals**

$$ Y_i = \sum_{k = 1}^K \gamma_k [ D_i - p ( X_i ) ] \mathbb{1} ( \hat{\ell}_{k - 1} \leq \hat{\tau} ( X_i ) < \hat{\ell}_k ) + \epsilon_i $$

with the model fitted via WLS using weights $w ( X_i)$.

##### GATES - **Horvitz-Thompson**

$$ H_i Y_i = \sum_{k = 1}^K \gamma_k \mathbb{1} ( \hat{\ell}_{k - 1} \leq \hat{\tau} ( X_i ) < \hat{\ell}_k ) + \epsilon_i $$

with the model fitted via OLS.

##### GATES - **AIPW**

$$ \hat{\Gamma}_i = \sum_{k = 1}^K \gamma_k \mathbb{1} ( \hat{\ell}_{k - 1} \leq \hat{\tau} ( X_i ) < \hat{\ell}_k ) + \epsilon_i $$

with the model fitted via OLS and the doubly-robust scores $\Gamma_i$ estimated via cross-fitting in the validation sample.

##### GATES - **Nonparametric**

$$ \hat{\gamma}_k = \frac{K}{\sum_{i = 1}^n D_i} \sum_{i = 1}^n Y_i D_i \mathbb{1} ( \hat{\ell}_{k - 1} \leq \hat{\tau} ( X_i ) < \hat{\ell}_k ) - \frac{K}{\sum_{i = 1}^n [ 1 - D_i ]} \sum_{i = 1}^n Y_i [ 1 - D_i ] \mathbb{1} ( \hat{\ell}_{k - 1} \leq \hat{\tau} ( X_i ) < \hat{\ell}_k ) $$

### Rank-Weighted Average Treatment Effects
The RATE induced by the estimated CATEs is defined as follows:

$$ \theta_{\alpha} ( \hat{\tau} ) := \int_0^1 \alpha ( u ) TOC ( u; \hat{\tau} ) d u  $$
where:

$$ TOC (u; \hat{\tau}) := \mathbb{E} [ Y_i ( 1 ) - Y_i ( 0 ) \mid F ( \hat{\tau} ( X_i )) \geq 1 - u ] - \mathbb{E} [ Y_i ( 1 ) - Y_i ( 0 ) ] $$
with $F ( \cdot )$ the cumulative distribution function of $\hat{\tau} ( \cdot )$, $0 < u \leq 1$, and $\alpha : ( 0, 1 ] \rightarrow \mathcal{R}$ a generic weight function.

The RATE provides a measure of the ability of our estimated CATEs to prioritize units to treatment in terms of intervention benefit. The idea is to regard $\hat{\tau} ( \cdot )$ as a "prioritization rule" that sorts units $i = 1, ..., n$ in order $j = 1, ..., n$ according to their estimated CATEs, for instance by prioritizing units with the largest estimated CATEs.[^6] [^7] 

We are interested in the estimation of the RATE for two reasons:

- $\theta_{\alpha} ( \hat{\tau}_1 ) > \theta_{\alpha} ( \hat{\tau}_2 )$ means that $\hat{\tau}_1 ( \cdot )$ produces more accurate CATE estimates than $\hat{\tau}_2 ( \cdot )$; 
- $\theta_{\alpha} ( \hat{\tau} ) = 0$ if either the effects are homogeneous or our CATE estimates are “bad." Thus, rejecting the hypothesis $\theta_{\alpha} ( \hat{\tau} ) = 0$ would imply that both the effects are heterogeneous and our CATE estimates are reliable.[^8]

The `evaluCATE` package estimates the TOCs and the RATE using the following sample-averaging estimators:

$$ \widehat{TOC} ( u; \hat{\tau} ) = \frac{1}{ \lfloor u n \rfloor } \sum_{j = 1}^{\lfloor u n \rfloor} \hat{\Gamma}_{i ( j )} - \frac{1}{n} \sum_{i = 1}^n \hat{\Gamma}_i $$
$$ \hat{\theta}_{\alpha} ( \hat{\tau} ) = \frac{1}{n} \sum_{j = 1}^n \alpha \left( \frac{j}{n} \right) \widehat{TOC} \left( \frac{j}{n}; \hat{\tau} \right) $$
where we let $i ( j )$ be the mapping from rank $j$ to unit $i$ (e.g., $i ( 1 )$ returns the most-prioritized unit, and $i ( n )$ returns the least-prioritized unit) and the doubly-robust scores $\Gamma_i$ are estimated via cross-fitting in the validation sample. Two different weight functions are considered, each corresponding to a different RATE:

- $\alpha ( u ) = 1$ &rarr; Area under the TOC curve (AUTOC)
- $\alpha ( u ) = u$ &rarr; Qini coefficient (QINI)

The half-sample bootstrap procedure is used to estimate the standard error of $\hat{\theta}_{\alpha} ( \cdot )$. In particular, the standard deviation of the bootstrap estimates can be used as an estimator of the standard error of $\hat{\theta}_{\alpha} ( \cdot )$.

## Code
The BLP, GATES, and RATEs can be estimated by calling the `evaluCATE` function. When calling this function, we need to supply our sample using the first six arguments: `Y_tr`, `Y_val`, `D_tr`, `D_val`, `X_tr`, and `X_val`, corresponding to $Y_i$, $D_i$, and $X_i$ separately for the training and validation subsamples. Additionally, we must supply our CATE predictions on the validation sample $\hat{\tau} ( X_i )$ by using the `cates_val` argument. Be careful, as these predictions must be produced by a model estimated using only the training sample.

The next two arguments allow us to choose our identification and estimation strategies for BLP and GATES (RATEs always estimated using the same estimator). Here we use all three strategies and do not add any denoising covariate. (note als GATES automatically implement nonparametric estimator).

We also have four optional arguments that we can use to supply predictins on the validation sample of the nuisance functions $p ( \cdot )$, $\mu ( \cdot )$, $\mu_0 ( \cdot )$, and $\mu_1 ( \cdot )$. Again, these predictions must be produced by models estimated using only the training sample. If not provided by the user, these functions are estimated internally via honest regression forests using only the training sample. In our Motivating Example, we have knowledge of the actual propensity score, which equals $0.5$ for all units. We supply these values in the call below and let the function estimate the other nuisances internally.[^9] 

Finally, we have four additional optional arguments. The first of these arguments controls the number of groups to be formed for the GATES analysis, with the default number equal to five. The second of these arguments controls the number of bootstrap replications used to estimate the standard error of the estimated RATEs. The third of these arguments controls how to rank units for the RATE estimation (according to either increasing or decreasing values of the estimated CATEs). The fourth of these arguments controls whether the `evaluCATE` function should print the status of progress on the console. We use the default of 5 groups, 200 bootstrap replications, the treatment is considered to be beneficial, and we prevent the function from printing the progresses.  

```{r call-main, eval = TRUE}
## Call main function.
strategies <- c("WR", "HT", "AIPW")
denoising <- "none"

pscore_val <- rep(0.5, length(Y_val)) # True propensity scores.

evaluation <- evaluCATE(Y_tr, Y_val, D_tr, D_val, X_tr, X_val, cates_val, strategies = strategies, denoising = denoising, pscore_val = pscore_val, verbose = FALSE)
```

Let us have a look at the results. The `summary` method allows us to visualize the results of the BLP and RATE estimation if the `target` argument is set to `BLP` or the results of the GATES estimation if the `target` argument is set to `GATES`. The `latex` argument controls whether the raw results or LATEX code for a table will be displayed in the console. We also use the `which_models` argument to choose which results to display (useful to avoid a lengthy output).

```{r blp-rates summary, eval = TRUE}
## BLP summary.
which_models <- c("wr_none", "ht_none", "aipw")
summary(evaluation, target = "BLP", which_models = which_models) # Try 'latex = TRUE'.
```

The estimated ATEs range between $0.05$ and $0.18$, with all confidence intervals covering the true value computed as `mean(mu1 - mu0)`. Additionally, we always fail to reject the hypothesis $\beta_2 = 0$, implying that either the effects are homogeneous or our CATE estimates are unreliable (we know both to be true in our example). The estimated RATEs agree with these findings.

```{r gates-summary, eval = TRUE}
## GATES summary.
summary(evaluation, target = "GATES", which_models = which_models) # Try 'latex = TRUE'.
```

All the GATES confidence intervals include the zero, suggesting that no group is affected by the treatment (true in our example!). The $p$-values attached to testing the hypotheses that all the GATES are equal and that the most affected and the least affected groups feature the same response to the treatment are large and lead to a failure in rejecting these hypotheses. To better digest GATES point estimates and confidence intervals, we can use the `plot` method setting the `target` argument to `"GATES"`.

```{r gates-plot, eval = TRUE, fig.dim = c(5, 3)}
## GATES plot.
plot(evaluation, target = "GATES", which_models = which_models)

```

Finally, we can look at the estimated TOC curve by calling the `plot` method setting the `target` argument to `"TOC"`. 

```{r toc-plot, eval = TRUE, fig.dim = c(5, 3)}
## TOC curve.
plot(evaluation, target = "TOC")
```

[^1]: Complete references to these papers are listed in the home page.
[^2]: Failing to reject this hypothesis means that either there is no heterogeneity or our CATE estimates are not reliable. Without additional evidence, we are not able to disentangle this. Check the [hypotheses testing vignette](https://riccardo-df.github.io/evaluCATE/articles/hypotheses-testing.html) for more details. 
[^3]: The linear regressions are used for estimation purposes. The identification hinges on linear projections defined at the population level, with the linear regressions constituting their sample analogs.
[^4]: Additional constructed covariates which are not necessary for identifying the targets but can significantly reduce the variance of the estimation can be included in the regressions. Details can be found in the [denoising vignette](https://riccardo-df.github.io/evaluCATE/articles/denoising.html).
[^5]: See footnotes 3 and 4.
[^6]: If the treatment is harmful, we prioritize units with the lowest estimated CATEs.
[^7]: Prioritization rules can be derived from other approaches, e.g., risk-based rules. Here we focus on CATE-based rules as we aim to use the RATEs to evaluate the quality of our estimated CATEs.
[^8]: See footnote 2.
[^9]: Notice that most methodologies implemented here are valid only under randomized experiments, where $p ( \cdot )$ is known.
